version: '3.8'

services:
  # Langflow - Visual flow builder for LLM agents
  langflow:
    image: langflowai/langflow:latest
    container_name: osmen-langflow
    ports:
      - "7860:7860"
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
      - LANGFLOW_CONFIG_DIR=/app/config
    volumes:
      - ./langflow/flows:/app/flows
      - ./langflow/config:/app/config
      - langflow-data:/app/data
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - osmen-network

  # n8n - Workflow automation
  n8n:
    image: n8nio/n8n:latest
    container_name: osmen-n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=changeme
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_USER=n8n
      - DB_POSTGRESDB_PASSWORD=n8n
      - EXECUTIONS_DATA_SAVE_ON_SUCCESS=all
      - GENERIC_TIMEZONE=UTC
    volumes:
      - ./n8n/workflows:/home/node/.n8n
      - n8n-data:/home/node/.n8n
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - osmen-network

  # LM Studio - Primary local LLM (run externally on host)
  # Access at: http://host.docker.internal:1234
  # Instructions: Install LM Studio on host and enable API server
  
  # Ollama - Secondary local LLM option (optional)
  ollama:
    image: ollama/ollama:latest
    container_name: osmen-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - osmen-network
    profiles:
      - ollama
    # Uncomment the following section if you have an NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
  
  # MCP Server - Model Context Protocol for tool integration
  mcp-server:
    build:
      context: ./gateway
      dockerfile: Dockerfile.mcp
    container_name: osmen-mcp-server
    ports:
      - "8081:8081"
    environment:
      - OBSIDIAN_VAULT_PATH=${OBSIDIAN_VAULT_PATH:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ${OBSIDIAN_VAULT_PATH:-./obsidian-vault}:/vault
      - ./tools:/app/tools
    restart: unless-stopped
    networks:
      - osmen-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Agent Gateway - Central API gateway for production LLM agents
  agent-gateway:
    build: ./gateway
    container_name: osmen-agent-gateway
    ports:
      - "8080:8080"
    environment:
      # OpenAI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      
      # GitHub Copilot Configuration
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      - COPILOT_API_URL=${COPILOT_API_URL:-}
      
      # Amazon Q Configuration
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      
      # Anthropic Claude Configuration
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      
      # LM Studio Configuration (host)
      - LM_STUDIO_URL=${LM_STUDIO_URL:-http://host.docker.internal:1234/v1}
      
      # Ollama Configuration (optional)
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      
      # Gateway settings
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ./gateway/config:/app/config
    restart: unless-stopped
    networks:
      - osmen-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Qdrant - Vector database for memory storage
  qdrant:
    image: qdrant/qdrant:latest
    container_name: osmen-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - osmen-network

  # PostgreSQL - Shared database for Langflow and n8n
  postgres:
    image: postgres:15-alpine
    container_name: osmen-postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - osmen-network

  # Redis - Caching and session management
  redis:
    image: redis:7-alpine
    container_name: osmen-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - osmen-network

volumes:
  langflow-data:
  n8n-data:
  ollama-data:
  qdrant-data:
  postgres-data:
  redis-data:

networks:
  osmen-network:
    driver: bridge
