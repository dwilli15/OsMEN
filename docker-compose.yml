services:
  # Langflow - Visual flow builder for LLM agents
  langflow:
    image: langflowai/langflow:latest
    container_name: osmen-langflow
    ports:
      - "127.0.0.1:7860:7860"
    environment:
      - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
      - LANGFLOW_CONFIG_DIR=/app/config
      - LANGFLOW_AUTO_LOGIN=true
      - LANGFLOW_SKIP_AUTH_AUTO_LOGIN=true
    volumes:
      - ./langflow/flows:/app/flows
      - ./langflow/config:/app/config
      - langflow-data:/app/data
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network

  # n8n - Workflow automation
  n8n:
    image: n8nio/n8n:latest
    container_name: osmen-n8n
    ports:
      - "127.0.0.1:5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-changeme}
      - N8N_HOST=${N8N_HOST:-localhost}
      - N8N_PORT=${N8N_PORT:-5678}
      - N8N_PROTOCOL=http
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${DB_POSTGRESDB_DATABASE:-n8n}
      - DB_POSTGRESDB_USER=${DB_POSTGRESDB_USER:-n8n}
      - DB_POSTGRESDB_PASSWORD=${DB_POSTGRESDB_PASSWORD:-n8n}
      - EXECUTIONS_DATA_SAVE_ON_SUCCESS=all
      - GENERIC_TIMEZONE=UTC
    volumes:
      - ./n8n/workflows:/home/node/.n8n
      - n8n-data:/home/node/.n8n
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network

  # LM Studio - Primary local LLM (run externally on host)
  # Access at: http://host.docker.internal:1234
  # Instructions: Install LM Studio on host and enable API server

  # Ollama - Secondary local LLM option (optional)
  ollama:
    image: ollama/ollama:latest
    container_name: osmen-ollama
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8G
        reservations:
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network
    profiles:
      - ollama
    # Uncomment the following section if you have an NVIDIA GPU
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G
    #     reservations:
    #       memory: 2G
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # MCP Server - Model Context Protocol for tool integration
  mcp-server:
    build:
      context: .
      dockerfile: gateway/Dockerfile.mcp
    container_name: osmen-mcp-server
    ports:
      - "127.0.0.1:8081:8081"
    environment:
      - OBSIDIAN_VAULT_PATH=${OBSIDIAN_VAULT_PATH:-}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Default to local embeddings for a local-first stack.
      - EMBED_PROVIDER=${EMBED_PROVIDER:-ollama}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - OPENAI_EMBED_MODEL=${OPENAI_EMBED_MODEL:-}
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - OLLAMA_HOST=${OLLAMA_HOST:-}
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-}
      - CHROMA_URL=${CHROMA_URL:-http://chromadb:8000}
    volumes:
      - ${OBSIDIAN_VAULT_PATH:-./obsidian-vault}:/vault:ro
      - ./tools:/app/tools:ro
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Agent Gateway - Central API gateway for production LLM agents
  agent-gateway:
    build:
      context: .
      dockerfile: gateway/Dockerfile
    container_name: osmen-agent-gateway
    ports:
      - "8080:8080"
    environment:
      # OpenAI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}

      # GitHub Copilot Configuration
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      - COPILOT_API_URL=${COPILOT_API_URL:-}

      # Amazon Q Configuration
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
      - AWS_REGION=${AWS_REGION:-us-east-1}

      # Anthropic Claude Configuration
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

      # LM Studio Configuration (host)
      - LM_STUDIO_URL=${LM_STUDIO_URL:-http://host.docker.internal:1234/v1}

      # Ollama Configuration (optional)
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}

      # PostgreSQL Configuration
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=${POSTGRES_DB:-osmen}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}

      # Gateway settings
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # External semester workspace (repo hygiene - semester artifacts MUST be outside repo)
      - OSMEN_SEMESTER_WORKSPACE=${OSMEN_SEMESTER_WORKSPACE:-}
    volumes:
      - ./gateway/config:/app/config
      # Bind-mount the external semester workspace so TTS/vault ops can write files
      - ${OSMEN_SEMESTER_WORKSPACE:-./semester_workspace}:/workspace
    restart: unless-stopped
    networks:
      - osmen-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ChromaDB - Unified Vector Database (replaces Qdrant)
  # All vector storage consolidated here: Librarian, agent memory, Obsidian sync
  chromadb:
    image: chromadb/chroma:latest
    container_name: osmen-chromadb
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - chroma-data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - PERSIST_DIRECTORY=/chroma/chroma
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped
    healthcheck:
      # chromadb/chroma image doesn't ship with curl/wget/python; use bash /dev/tcp.
      test:
        [
          "CMD-SHELL",
          "bash -lc \"exec 3<>/dev/tcp/127.0.0.1/8000 && printf 'GET /api/v2/heartbeat HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && cat <&3 | head -n 1 | grep -q '200'\"",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - osmen-network

  # PostgreSQL - Shared database for Langflow and n8n
  postgres:
    image: postgres:15-alpine
    container_name: osmen-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-postgres}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - osmen-network

  # Redis - Caching and session management
  redis:
    image: redis:7-alpine
    container_name: osmen-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - osmen-network

  # ConvertX - Universal file format converter (1000+ formats)
  # Supports: video, audio, images, documents, 3D models, ebooks, etc.
  # Web UI: http://localhost:3000
  convertx:
    image: ghcr.io/c4illin/convertx:latest
    container_name: osmen-convertx
    ports:
      - "127.0.0.1:3000:3000"
    environment:
      - JWT_SECRET=${CONVERTX_JWT_SECRET:-osmen-convertx-secret-key-change-me}
      - ACCOUNT_REGISTRATION=true
      - HTTP_ALLOWED=true
      - ALLOW_UNAUTHENTICATED=true
      - AUTO_DELETE_EVERY_N_HOURS=24
      - HIDE_HISTORY=false
    volumes:
      - convertx-data:/app/data
      - ./content:/app/content:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G
        reservations:
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network

  # NOTE: Web dashboard removed - use n8n (:5678) and Langflow (:7860) instead

  # Librarian - Semantic Memory & Lateral Thinking RAG Engine
  librarian:
    build:
      context: ./agents/librarian
      dockerfile: Dockerfile
    container_name: osmen-librarian
    ports:
      - "127.0.0.1:8200:8200"
    environment:
      - LIBRARIAN_DATA_DIR=/app/data
      - LIBRARIAN_DB_PATH=/app/data/db
      - LIBRARIAN_EMBEDDING_MODEL=${LIBRARIAN_EMBEDDING_MODEL:-dunzhang/stella_en_1.5B_v5}
      - LIBRARIAN_API_PORT=8200
      - RAG_DEFAULT_MODE=${RAG_DEFAULT_MODE:-lateral}
      - RAG_TOP_K=${RAG_TOP_K:-5}
      - RAG_MMR_LAMBDA=${RAG_MMR_LAMBDA:-0.5}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - librarian-data:/app/data
      - ./data/librarian:/app/data/sources:ro
    depends_on:
      postgres:
        condition: service_healthy
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8G
        reservations:
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - osmen-network
    profiles:
      - librarian

volumes:
  langflow-data:
  n8n-data:
  ollama-data:
  chroma-data:
  postgres-data:
  redis-data:
  librarian-data:
  convertx-data:

networks:
  osmen-network:
    driver: bridge
