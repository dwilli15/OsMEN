{
  "agents": {
    "priority_order": [
      "openai",
      "copilot",
      "amazonq",
      "claude",
      "lmstudio",
      "ollama"
    ],
    "openai": {
      "enabled": true,
      "name": "OpenAI (Codex, GPT-4)",
      "description": "OpenAI's production language models",
      "api_type": "openai",
      "default_model": "gpt-4",
      "required_role": "admin",
      "models": [
        "gpt-4",
        "gpt-4-turbo-preview",
        "gpt-3.5-turbo",
        "gpt-4-32k"
      ],
      "capabilities": ["code", "chat", "reasoning", "analysis"]
    },
    "copilot": {
      "enabled": true,
      "name": "GitHub Copilot",
      "description": "GitHub's AI pair programmer",
      "api_type": "copilot",
      "required_role": "operator",
      "integration_methods": [
        "vscode",
        "cli",
        "api"
      ],
      "capabilities": ["code_completion", "code_generation", "refactoring"]
    },
    "amazonq": {
      "enabled": true,
      "name": "Amazon Q",
      "description": "Amazon's generative AI assistant",
      "api_type": "aws",
      "required_role": "admin",
      "integration_methods": [
        "aws_cli",
        "sdk",
        "web_ui"
      ],
      "capabilities": ["code", "chat", "aws_expertise"]
    },
    "claude": {
      "enabled": true,
      "name": "Anthropic Claude",
      "description": "Anthropic's Claude models",
      "api_type": "anthropic",
      "default_model": "claude-3-opus-20240229",
      "required_role": "admin",
      "models": [
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307"
      ],
      "capabilities": ["code", "chat", "reasoning", "long_context"]
    },
    "lmstudio": {
      "enabled": true,
      "name": "LM Studio",
      "description": "Primary local LLM runtime",
      "api_type": "openai_compatible",
      "host_url": "http://host.docker.internal:1234",
      "installation": "Install LM Studio on host and enable API server",
      "capabilities": ["local_inference", "privacy", "offline"],
      "required_role": "viewer"
    },
    "ollama": {
      "enabled": false,
      "name": "Ollama",
      "description": "Secondary local LLM runtime (optional)",
      "api_type": "ollama",
      "default_model": "llama2",
      "required_role": "operator",
      "models": [
        "llama2",
        "mistral",
        "codellama",
        "deepseek-coder"
      ],
      "capabilities": ["local_inference", "privacy", "offline"]
    }
  },
  "routing": {
    "default_agent": "openai",
    "fallback_agent": "lmstudio",
    "route_by_task": {
      "code_generation": ["copilot", "openai", "lmstudio"],
      "code_review": ["openai", "claude", "lmstudio"],
      "reasoning": ["openai", "claude", "lmstudio"],
      "chat": ["openai", "claude", "lmstudio"],
      "aws_tasks": ["amazonq", "openai"],
      "local_only": ["lmstudio", "ollama"]
    }
  }
}
